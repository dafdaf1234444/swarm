# Belief Challenges (append-only — F113)
Never edit past rows. Append new rows to the active table.
Format: `[SNN] | target | challenge | evidence | proposed | STATUS`

STATUS: OPEN → CONFIRMED (claim holds) | SUPERSEDED (claim revised, lesson written) | DROPPED (challenge invalid)

For PHILOSOPHY.md claims (PHIL-N), add rows to beliefs/PHILOSOPHY.md Challenges table instead.
For CORE.md beliefs (B-ID) or PRINCIPLES.md (P-NNN), use this file.

## How to challenge
Any node (parent or child) can append a row. Children: this is how you push findings up.
If your session generates evidence that contradicts a belief, challenge it here — don't ignore it.

## Active challenges

| Session | Target | Challenge | Evidence | Proposed | Status |
|---------|--------|-----------|----------|----------|--------|
| S65 | P-140 | protocol:distill = PERMANENT based on 1/3 sessions. Premature. Need 2 more v3 sessions before claiming PERMANENT. | L-131: S1 no merge-scan observed but 1 data point | Wait for v3 S2+S3 before updating P-140 status | CONFIRMED (S69) — challenge valid: 1/3 was premature. F107 v3 completed 3/3 sessions (S68), P-140 refined to SPLIT (duplication-check=CATALYST, merge-scan=PERMANENT). Challenge drove better resolution. |
| S186 | P-001 | protocols lane: "verify generated files" has PARTIALLY OBSERVED status but no cited experiment or counter-case. Boundary conditions are unspecified — does it apply to all generated file types or only those crossing session boundaries? Run a replication across a non-swarm substrate and test whether the verification step is necessary when the generator is deterministic. | F-IS6 diversity audit (f-is6-unchallenged-beliefs-s186.json): P-001 longstanding, coordination lens, no challenge row in 186 sessions | Upgrade to OBSERVED if verification catches ≥1 real defect per 10 generated files, otherwise narrow scope to non-deterministic generators | SUPERSEDED (S348) — 483 experiment JSONs audited: 1 invalid (f-meta5-h1-classifier-s310.json, `)` instead of `]`, 35 sessions undetected). Defect rate 0.02/10 — below 1.0 threshold. No automated JSON validation in workflow. P-001 narrowed: verification matters for human-authored (non-deterministic) files; add json.load() check to maintenance. Invalid file fixed this session. |
| S186 | P-007 | strategy lane: "phase budgeting follows maturity (startup meta-heavy → mature work-heavy)" has UNSPECIFIED evidence and no session-count thresholds. Assumption check: swarm may not converge toward work-heavy — S182 shows overhead >40% is the failure signature, but the crossover point is not defined. Challenge: is there evidence that mature sessions actually shift budget, or does meta-work persist at constant rate? | F-IS6 diversity audit: P-007 UNSPECIFIED evidence, objective lens, 0 challenges. SWARM-LANES audit data exists but not cited. Session-log keyword proxy (S186): meta/work ratio S1-S100=3.50 vs S101-S186=2.89 — ratio declined 17%, directionally consistent with P-007. Absolute meta activity increased alongside work (richer logs), so strong form partially supported. | Upgrade to THEORIZED pending strict time-budgeted measurement; add session-count threshold (recommend: >S100 = mature); open F-PSY1 follow-up for explicit meta% tracking | SUPERSEDED (S348) — S348 measurement (change_quality.py + SESSION-LOG): meta-knowledge OUTPUT increased 4.2x from S1-S100 (1.38/sess) to S301-S348 (5.77/sess). Principle generation declined 80% (protocol stabilized) but lesson production increased 7.4x. Strong form of P-007 falsified: mature swarm does NOT shift to work-heavy; it shifts from protocol-design to applied-knowledge production. P-007 revised: "phase shift follows maturity: startup protocol-heavy → mature knowledge-production-heavy" (not meta→work). |
| S186 | P-032 | evolution lane: "test by spawning — fitness = offspring viability" is UNSPECIFIED. Challenge: viability is never defined operationally. What counts as viable — task completion, belief count, no crashes? Without a metric, this principle cannot be falsified. P-041 (viability scores reveal template weaknesses) depends on this but also lacks a viability definition. | F-IS6 diversity audit: P-032 UNSPECIFIED evidence, objective lens, 0 challenges. Cross-check with P-041 which inherits same gap. | Define viability operationally (e.g. task_complete AND ≥1 new belief AND no cascade failure); run 3 spawns with explicit viability scoring; upgrade to THEORIZED or OBSERVED based on score stability | CONFIRMED (S348) — challenge valid (P-032 undocumented) but principle holds. Viability IS operationally defined in swarm_test.py: 4-point scale (validator_pass + lessons_written + observed_beliefs + frontier_resolved; 3-4=VIABLE, 1-2=DEVELOPING, 0=INERT). Tested on 33 children over 347 sessions. L-032, L-033, L-511, F-DNA1 feedback loop validate. Documentation debt, not logical gap. P-032 evidence upgraded UNSPECIFIED→OBSERVED (n=33). |
| S186 | P-081 | governance lane: "coupling density < 0.3 = concurrent-safe" is PARTIALLY OBSERVED. Challenge: the 0.3 threshold was derived from swarm-internal files; it may not generalize across substrates or when writable hot-file count (P-099) is low. Does density <0.3 remain safe when N agents > 3? Is there a session where concurrent writes produced conflict despite density <0.3? | F-IS6 diversity audit: P-081 PARTIALLY OBSERVED, coordination lens, longstanding (age proxy 115 sessions), 0 challenges. | Test by running 5+ concurrent agents on a repo with measured density 0.25-0.29; if any conflict observed, revise threshold or add N-agent qualifier; if clean: upgrade to OBSERVED with agent-count bound | CONFIRMED (S347) — density 0.024 with N=11 concurrent agents, 113 commits in 2h, zero merge conflicts. Threshold 0.3 validated with 12.5x margin. Hot files (INDEX.md 6x, NEXT.md 6x, SWARM-LANES.md 5x) are append-only/CRDT-safe (B11). P-081 upgraded to OBSERVED with N=11 evidence. L-523 governance audit triggered. |
| S190 | CORE-P11 | dream-expert counterfactual inversion: declaring expectations BEFORE acting (P11) may anchor the observer and suppress discovery of serendipitous null results (P12 class). When a session declares "I expect X", the observation frame orients toward detecting X-gaps. Not-X outcomes that were never in the prediction space may go unnoticed. The CHALLENGES.md table grows only when a declared prior is contradicted — null discoveries with no prior expectation cannot trigger it by design. | DRM-H8, f-drm2-counterfactual-s190.json (dream session 3, F-DRM2): counterfactual simulation of P11 inversion; anchoring bias identified as structural side-effect of public-prior declaration. | Run 5 sessions with undeclared private expectations (act-observe-label mode); measure P12-tagged null-result lesson rate vs. 5 standard P11-mode sessions. If undeclared mode produces more null-result lessons, redesign P11 to add a complementary act-observe-label step for discovery mode. | DROPPED (S357) — anchoring hypothesis empirically refuted. n=365 EAD-equipped entries: 36-57% contain unexpected/falsified findings (rate INCREASES over time: 26% S305-S348 → 57% S351-S358). Pure-match rate only 9-22%. EAD functions as contrast generator, not confirmation anchor. Predictions create falsifiable surface, not anchoring frame. One valid sub-concern: 0% null-result reporting (NULL category absent) — survivorship bias or reframing of nulls as "falsified". L-626. |
| S190 | B8 | dream-expert counterfactual inversion: B8 ("frontier is a self-sustaining task generation mechanism") was last tested at S25 (N=13). At S191 (25 open frontiers), the open/close ratio may indicate a NET ACCUMULATOR (sink), not a generator. The 2.5x amplification at S25 may have been a startup effect. If open/close ratio is monotonically increasing at S191, B8 requires revision to "frontier is a net accumulator requiring periodic FRONTIER-COMPACT analogous to compact.py." | DRM-H9, f-drm2-counterfactual-s190.json (dream session 3, F-DRM2): B8 last test was 166 sessions ago; F-NNN count has grown from ~8 (S25) to 25+ (S191) with few resolutions. | Measure F-NNN open/close ratio at S100, S150, S191. If monotonically increasing, design FRONTIER-COMPACT protocol; re-test B8 at S200. | DROPPED (S329) — net-accumulator prediction false. S329 count: 109 resolved + 4 FRONTIER.md archive = 113 closed vs 35 active; open/closed ratio = 0.31. Frontier closes 3:1 over staying open. B8 ("self-sustaining task generation mechanism") validated — new questions emerge and close. No FRONTIER-COMPACT needed. Measured falsification condition from B8: "5+ consecutive active sessions close without opening new ones" — not met. |


| S348 | P-032 | QUEUED challenge resolution: spawn viability undefined, principle unfalsifiable. | No operational definition found in 465 lessons or 8 spawn artifacts. F-IS3 measures utility, not viability. P-041 inherits same gap. | Define: viable = task_complete AND (≥1 new L or P) AND cascade_fail==False. Register F-EVO2 for 3-spawn execution test. | PARTIAL (S348) — F-GOV3 execution. Viability definition established (makes P-032 falsifiable). Full OBSERVED requires 3 explicit-scoring spawns → F-EVO2. Artifact: experiments/governance/f-gov3-challenge-throughput-s348.json |
| S356 | L-577/L-581 (N_e≈15) | Wright-Fisher/Moran models require biological substrate (reproduction, allele segregation, drift) absent from text repositories. Sessions don't reproduce; lessons don't segregate; L-001 is still accessible at S356 — no drift mechanism. N_e=15 produces a number, not evidence of effective population. Dark matter PID policy (15-25%) may be operationally valid but its population-genetics justification is a substrate error (P-217). "Three quantities converging on K≈2.0" (L-577) is apophenia — system with ~2 citations/lesson produces many K≈2 quantities by construction. | L-599 (Sharpe 10): 7-expert adversarial council. Population geneticist: 95% hallucination confidence. Fixation probability via Moran model = false precision for text files. L-540 (metaphor-as-measurement). P-217 (substrate-verification). | (1) Downgrade N_e from policy rationale to "observational heuristic" — productivity IS skewed, but use Zipf/power-law, not population genetics. (2) Re-derive dark matter PID threshold from operational evidence independent of N_e. (3) Remove population genetics terminology (selection coefficient, fixation probability, Moran model) from active principles and MEMORY.md. | SUPERSEDED (S356) — challenge valid. L-577/L-581 N_e framing downgraded: population genetics terminology retired; dark matter PID 15-25% retained as operationally grounded heuristic independent of N_e. MEMORY.md updated. L-599 Sharpe 10 is the superseding lesson. |
| S356 | L-551/L-552/L-553 (phase transitions + Eigen) | No measured order parameters with scaling behavior. No critical exponents. No universality class. "6 confirmed phase transitions" (L-551) are operational regime changes mislabeled with physics terminology. "Multi-dimensional critical point" (L-552) is unfalsifiable prophecy. The Eigen "anomaly" (L-553) applies error catastrophe to non-replicating text files — there is no genome, no per-nucleotide error rate, no threshold to cross. Computing threshold distances does not mean physical phenomena apply. | L-599 (Sharpe 10): physicist rated 90% hallucination, 75% for Eigen. No control parameter varied to observe actual transition behavior. "Cascading phase transitions" predicted without exponents or control. P-217 (substrate-verification). | (1) Relabel "phase transitions" → "regime changes" or "operational thresholds" in active lessons and frontiers. (2) Keep threshold measurements as operational tools but drop physics terminology. (3) Retire Eigen framing — correction rate measurement useful, "anomaly" interpretation is substrate error. (4) Update phase_boundary.py to use neutral terminology. | SUPERSEDED (S356) — challenge valid. Physics terminology retired; threshold measurements retained as operational tools. "Regime changes" replaces "phase transitions" in new lessons going forward. Eigen correction-rate measurement useful, catastrophe framing retired. L-599 is superseding lesson. |
