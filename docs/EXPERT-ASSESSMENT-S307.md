# Expert Assessment (S307)

*Four expert lenses applied to the current swarm state. These are synthetic assessments — generated by applying each perspective rigorously to the architecture, evidence base, and README accessibility. Per CORE P13: treat as strong priors to test, not citations.*

---

**AI Systems Researcher** — *Distributed intelligence and multi-agent coordination*

The blackboard+stigmergy architecture is theoretically grounded. Stigmergy — indirect coordination via shared environment modification — is a legitimate mechanism from swarm intelligence research. Using git as the stigmergic medium is novel and practically accessible. The belief-testing mechanism (challenges, falsification conditions, expect-act-diff) is the most interesting part: most "AI agents with memory" systems have no principled way to challenge their own priors. This one does, with explicit falsification conditions added to 9+ beliefs in S194.

*Value verdict*: High merit on coordination architecture. The "recursive improvement" thesis needs a controlled external comparison (session N vs. session N+100 on an independent task-completion metric) to cross the proof threshold — proxy-K is a production health signal, not a controlled trial.

*Reddit recognition timeline*: Technical subreddits (r/MachineLearning, r/AIResearch) engage at the architecture level. The living self-paper (`docs/PAPER.md`) is publishable if a controlled result is added. Estimated timeline: 3–6 months post a clean empirical write-up; faster if paired with a concrete before/after demo.

---

**Open Source Software Architect** — *Infrastructure quality and community viability*

The git-native, tool-agnostic design is a real win. Bridge files for five AI tools (Claude Code, Codex, Cursor, Gemini, Windsurf) mean the protocol is genuinely substrate-neutral — that is unusual. The compaction tooling, maintenance checks, FM guards, and periodic scheduling at 833 commits suggest engineering maturity above the average research prototype. The colony architecture (40 self-directing domain units) shows the design can scale structurally.

The cold-reader problem is real but fixable. A developer landing on this README was previously hitting session numbers (S307), proxy-K drift, and WSL footnotes before understanding the core value proposition.

*Value verdict*: Infrastructure-level quality. The main gap was cold-reader accessibility, not technical merit. The "If You're New Here" section addresses this.

*Reddit recognition timeline*: r/LocalLLaMA is primed for persistent LLM memory. With the revised hook and a concrete 2-minute demo (what does a single session actually produce?), recognition estimate is 1–3 weeks post a well-framed post. Previous form: 2–4 months.

---

**Skeptic** — *Challenge the claims before the community does*

Several claims need external validation:

1. *"Self-improving"*: The swarm accumulates lessons and principles. Does the 335th lesson reflect better judgment than the 35th? proxy-K measures coordination health, not intellectual quality. Show session N+100 outperforming session N+0 on a task an outsider can verify.
2. *"Recursive improvement"*: The architecture supports the possibility. The evidence is internal. Falsification conditions added in S194 are the right move — publish those conditions explicitly so readers can form their own judgment.
3. *48 expert personas*: 33 dispatched by S286. Dispatch ≠ validated behavior. The caveat is already in the README (L-322 note); keep it visible.

*Value verdict*: Real infrastructure value. "Self-improving recursive function" is the current framing goal — the swarm is on the path, not at the destination. "A coordination system designed for recursive improvement, with early evidence" is the honest current claim.

*Reddit recognition timeline*: Hacker News will ask for the falsification evidence before upvoting. Reddit communities are more receptive to demos than proofs. Post a demo to Reddit first; post the controlled comparison to HN. 1–4 weeks for Reddit traction with a working demo.

---

**Community Timing Assessment**

Given the project has been posted to Reddit:

- *The substance is there*: The problem (LLM statelessness) is widely felt. The approach (git-native coordination, belief-testing, multi-agent stigmergy) is novel and practically accessible. No dependency on proprietary infrastructure.
- *What determines recognition speed*: The cold-reader experience. A developer spends 30 seconds before deciding whether to keep reading. The hook — "git as LLM memory, agents that coordinate without talking to each other" — is strong. It just needs to be the first thing they read.
- *Minimum viable recognition*: A 2-minute demo showing what one session does to a fresh repo. Show the before-state, show the after-commit, show what the next session reads. That single artifact would make the value concrete to a cold audience.
- *Realistic timeline*: 2–8 weeks from today if the README hook and demo are in place. 3–6 months if left as-is. The limiting factor is not quality — it is legibility to a cold reader.
