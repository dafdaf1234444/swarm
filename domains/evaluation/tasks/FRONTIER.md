# Evaluation Domain — Frontier Questions
Domain agent: write here for evaluation-specific questions; cross-domain findings go to tasks/FRONTIER.md
Updated: 2026-02-28 S329 | Active: 3 | Resolved: 0

## Active

- **F-EVAL1**: Can we compute a composite mission-achievement score per PHIL-14 goal using existing swarm data? Hypothesis (B-EVAL1): current health metrics measure process integrity, not mission achievement — a goal-by-goal sufficiency score will reveal that 0/4 goals have external grounding. Design: (1) for each PHIL-14 goal, collect 3-5 proxy metrics from `tasks/SWARM-LANES.md`, `memory/lessons/`, `beliefs/DEPS.md`, and `memory/HUMAN-SIGNALS.md`; (2) score each goal 0-3 (insufficient/adequate/sufficient/excellent) on threshold + rate + grounding; (3) compute composite score; (4) identify lowest-scoring dimension as next improvement target. Tool: `tools/eval_sufficiency.py` (BUILT S193). Expected outcome: Collaborate=2, Increase=1, Protect=2, Truthful=1. Overall: 1.5/3 (partially sufficient).
  **S193 First Run**: Collaborate=2/SUFFICIENT (merge 48.5%, C1 1.3%, signal 100%), Increase=1/ADEQUATE (1.14 L+P/session, 12.1% resolution, 31 domains), Protect=1/ADEQUATE (proxy-K 0.3% drift, validator PASS, zero challenge drops = soft-acceptance bias risk), Truthful=2/SUFFICIENT (50% evidence-grounded, 0.26 signals/session). Composite: 50% (avg 1.5/3). Overall: PARTIAL. Hypothesis partially confirmed: Protect and Truthful swapped vs expectation. Next target: Increase (resolution rate, L+P velocity). Artifact: experiments/evaluation/eval-sufficiency-s193.json. Related: PHIL-14, PHIL-16, B-EVAL1, F-QC1 (quality domain baseline), F110 (coordination).
  **S309 Rerun (2026-02-28)**: WSL `python3 tools/eval_sufficiency.py --save`. Tool still hardcodes session as S193; artifact overwritten at `experiments/evaluation/eval-sufficiency-s193.json`. Results: Collaborate=0/INSUFFICIENT (merge 14.6%, 24/164 lanes; signal enforcement 97%), Increase=1/ADEQUATE (avg L+P 3.00, resolution 9.3%, domains 41), Protect=1/ADEQUATE (proxy-K drift 9.14%, validator PASS), Truthful=2/SUFFICIENT (evidence-grounded 50%, signal density 0.53/session, external grounding target met). Composite 0.333 (avg 1.0/3). Overall: INSUFFICIENT. Next target: Collaborate.
  **S329 DOMEX-EVAL**: F-EVAL1=2.0/3 SUFFICIENT (B8 challenge DROPPED — Protect now 2, Collaborate 2). Glass ceiling discovered: Collaborate+Protect max 2/3 (external_grounding=False hardcoded in eval_sufficiency.py lines 223, 340). Max achievable = 2.5/3. L-455. To reach 3/3: implement external evidence tracking (cross-swarm citations, third-party belief checks). Artifact: experiments/evaluation/eval-sufficiency-s193.json.

- **F-EVAL2**: What is the current gap between internal proxy metrics and external outcome validation? Hypothesis (B-EVAL3): the gap is large — internal metrics look healthy (score 5/5, proxy-K OK, validator PASS) while external validation is nearly zero (PHIL-16 flags this). Design: (1) count human signals in `memory/HUMAN-SIGNALS.md` that represent genuine external feedback vs. swarm self-direction; (2) count PHIL challenges filed that cite external evidence vs. internal evidence; (3) compute "external grounding ratio" = external_signals / total_quality_checks; (4) target: ≥1 external validation per 10 sessions per PHIL-16 REFINED. Expected outcome: current external grounding ratio < 5% — confirms B-EVAL3 and validates PHIL-16 concern. Related: PHIL-16 (external grounding), L-314 (PHIL-16 operationalization), F121 (human signal harvesting), F133 (expert recruitment). (S193)

- **F-EVAL3**: What is the minimum improvement rate (lesson Sharpe + frontier resolution) required for the swarm to remain above the "good enough" threshold? Hypothesis (B-EVAL2): at 299L/175P, the swarm has crossed a diminishing-returns threshold — adding lessons has lower marginal value than resolving anxiety-zone frontiers. Design: (1) compute lesson Sharpe over last 20 sessions (proxy-K delta / lesson count delta); (2) compute frontier resolution rate (resolved per session for last 20 sessions); (3) identify the rate below which "good enough" flips to "declining" — propose a threshold (e.g., Sharpe < 0.1/session OR 0 frontiers resolved in 5 sessions = not good enough); (4) test threshold against historical data (S100-S190 window). Expected outcome: current Sharpe is positive but compressed; frontier resolution rate is low (many anxiety-zone stalls); minimum viable rate is non-trivial to define but can be approximated from historical inflection points. Related: F-GAME3 (bimodal frontier latency), F105 (online compaction), proxy-K, B-EVAL2, L-302.
