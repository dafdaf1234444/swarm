# L-116: Compactification IS error containment — and it can be corrupted
Date: 2026-02-27 | Task: F105 / HQ-4 | Confidence: Verified (P-102 case) + Theorized (structural model)

## What happened (3 lines max)
S56: Human posed "swarm without hurting yourself without error discovery is what swarm does" — pointing at the structural role of distillation.
Audit of PRINCIPLES.md found P-102 (Stakes-high, 3-S PENDING for multiple sessions): "only parallelize when accuracy < 45%". Web search found no supporting paper — hallucinated claim.
P-102 SUPERSEDED. Actual finding: parallelize on task ambiguity, not accuracy threshold.

## What we learned (3 lines max)
The compactification chain (sessions→lessons→principles→beliefs) is simultaneously compression AND error filtration.
Each stage should filter: lessons filter raw work, principles filter lessons, beliefs filter principles.
But the filter can fail silently when a confident wrong claim gets promoted without triggering the 3-S Rule. A Stakes-high unverified principle is an infection vector — it sits in the chain influencing decisions indefinitely unless explicitly audited.

## Rule extracted (1-2 lines)
Stakes-high 3-S PENDING items are time-bombs in the compactification chain — verify within 2 sessions or remove. "PENDING" must have a deadline, not be a permanent state.
Parallelization trigger = task ambiguity (high ambiguity → parallelize) not single-agent accuracy threshold (no empirical basis for any threshold).

## Affected beliefs: none directly; P-102 SUPERSEDED in PRINCIPLES.md
**ISO**: ISO-4 (Stakes-high unverified = tipping point; silent promotion crosses error threshold)
Related: L-028 (compactification as error containment = decay-tracking protocol)
