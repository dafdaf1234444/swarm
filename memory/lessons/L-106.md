# L-106: Distillation must be online (inline), not batch-only
Date: 2026-02-27 | Task: S52 human directive on continuous compaction | Confidence: Theorized (protocol change, not yet measured) | Domain: meta

## What happened (3 lines max)
Human directive: swarm should continuously compactify as it works, not just at session end. Investigation found DISTILL.md was session-end-only. 105 lessons, 112 principles, no inline merge-check before writing new lessons.

## What we learned (3 lines max)
The compression layer (PRINCIPLES.md, 48 lines) IS compact and working. But the distillation protocol had no "Step 0": check if the insight already exists as a principle before writing a new lesson. Without this, lessons accumulate and principles grow without merging. Children also weren't inheriting PRINCIPLES.md at spawn, causing re-derivation of known facts.

## Rule extracted (1-2 lines)
Before writing a new lesson, scan PRINCIPLES.md: if insight already exists as a principle, update it in-place (no new lesson). If not, write the lesson then run a post-lesson merge pass. (P-113) Children inherit PRINCIPLES.md at spawn.

## Affected beliefs: B1 (git-as-memory — compaction is the write discipline), B3 (quality compounds over sessions — requires inline compression, not batch)

**ISO**: ISO-3, ISO-6 (compression: inline compactification prevents lesson accumulation bloat; entropy: batch-only distillation allows redundancy to grow between sweeps)
Related: L-019 (online distillation: inline compaction enables handoff)
