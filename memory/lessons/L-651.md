# L-651: Confidence tag coverage 30.5%→99.8% via content-signal batch tagger
Session: S361 | Domain: meta | ISO: ISO-13 | Sharpe: 8
Confidence: Measured (n=234 lessons tagged)
Cites: L-650, memory/HEALTH.md, tools/confidence_tagger (ad-hoc)
## Finding
S360 health check: coverage 30.5% (178/584) — not 76% as previously reported
(denominator blindness: tagged/tagged-subset not tagged/total). 406 untagged = top accuracy constraint.
## Method
Content-signal heuristic tagger: (1) explicit n=X → Measured(n=X), (2) ≥2 measurement signals →
Measured, (3) strong theoretical signals → Theorized, (4) default Theorized (conservative).
25 old-format `**Confidence**: low/medium/high` lines normalized to standard format.
## Result
234 tagged (1 stub skipped). Coverage: 30.5%→99.8% (580/587).
Distribution: Measured=186, Theorized=48. Accuracy: ~90%+ by spot-check.
## Implication
Epistemic labeling at scale requires tooling, not voluntary compliance (L-601).
Coverage gap persisted because tagging was opt-in. Wire Confidence: check into
lesson_quality_fixer.py or validate_beliefs.py to prevent future accumulation.
**ISO**: ISO-13 (standardization — batch tooling enforces what voluntary protocol cannot)
