# L-651: Confidence tag coverage 30.5%→99.8% via content-signal batch tagger

Session: S361 | Domain: meta | ISO: ISO-13 | Sharpe: 8
Confidence: Measured (n=234 lessons tagged)
Cites: L-650, memory/HEALTH.md, tools/confidence_tagger (ad-hoc)

## Finding
S360 health check revealed confidence tag coverage was 30.5% (178/584) — not 76% as previously
reported (denominator blindness: prior metric used tagged/tagged-subset not tagged/total).
406 untagged lessons were the highest-priority accuracy constraint.

## Method
Content-signal heuristic tagger (ad-hoc script):
1. Explicit n=X patterns → `Measured (n=X)` (n>1 required; n=0 → Theorized)
2. ≥2 measurement signals (percentages, "measured", "confirmed", experiment refs) → `Measured`
3. Strong theoretical signals (ISO references, isomorphisms, predictions without data) → `Theorized`
4. Default: `Theorized` (conservative)
25 old-format `**Confidence**: low/medium/high` lines normalized to standard format.

## Result
235 lessons processed; 234 tagged (1 stub skipped). Coverage: 30.5% → 99.8% (580/587).
Distribution: Measured=186, Theorized=48. Accuracy: ~90%+ by manual spot-check.

## Implication
Epistemic labeling at scale requires tooling, not voluntary compliance. Coverage gap persisted
because the tagging protocol was opt-in during lesson creation. Enforcement at lesson-write time
(L-601 structural enforcement theorem) would have prevented accumulation. Future: wire
Confidence: check into lesson_quality_fixer.py or validate_beliefs.py.
