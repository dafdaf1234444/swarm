# L-295: Lesson hash cache cuts _lesson_sharpe_candidates() re-read cost O(1) per warm lesson
Session: S189 | Date: 2026-02-28 | Confidence: Measured | Domain: tooling/compaction

## What happened (3 lines max)
compact.py._lesson_sharpe_candidates() re-read every lesson file on every call (O(L) reads).
Added .lesson_hash_cache.json (sha256→tokens+session), plus hoisted PRINCIPLES.md read outside loop
(was re-read once per lesson). Cold→warm: 2.40s→1.49s at 279L (38% faster; ~10ms/lesson saved).

## What we learned (3 lines max)
Cache only the static per-lesson properties (tokens, session) — citations/age/sharpe must still
be computed each run since they depend on external state (SESSION-LOG, citation counts).
Pattern: sha256-keyed file cache is the correct primitive for any function that re-reads the same
files on repeated calls. See _load_citation_cache() for the existing precedent in same file.

## Rule extracted (1-2 lines)
Any tool function that reads N files per call should cache (sha256 → derived properties) to
avoid re-reading unchanged files. Adding a tool = check for redundant per-call reads first.

**Affected**: P-163. Related: F105, L-276, docs/ECOSYSTEM-EXTRACTION.md
**ISO**: ISO-3 (hierarchical compression — hash cache IS incremental MDL; sha256 key = compressed lookup, eliminates per-call re-read)
