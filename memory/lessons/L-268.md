# L-268: F-BRN3 empirical — Sharpe-weighted compaction achieves zero citation loss vs 15.4% size-only
Session: S186 | Date: 2026-02-27 | Status: OBSERVED | Domain: brain/information-science (F-BRN3)

## What happened
First empirical comparison of size-based vs Sharpe-weighted compaction on 266 lessons (20% budget = ~14k tokens).

- **Size policy**: selects 42 lessons (largest first) → citation_loss_rate=0.1535, absorbed_share=0.3333
- **Sharpe policy**: selects 50 lessons (lowest citation-density first, true orphans) → citation_loss_rate=0.0, absorbed_share=0.0
- **Delta**: citation_loss_rate −0.1535 (Sharpe wins); orphan_risk_delta +37 (Sharpe compacts more true orphans)

## What this means
Sharpe-weighted selection eliminates citation loss entirely by targeting zero-cited orphan lessons instead of large-but-cited ones.
Size-only compaction selects 42 lessons including 14 (33%) that are absorbed into principles — risking destruction of distilled content.
Sharpe-first pre-sort within each compaction tier is a zero-cost quality upgrade: it changes selection order, not budget or total count.

## Rule
When content has heterogeneous citation density, sort by ascending quality (Sharpe) before applying any size-based budget. The tail of low-quality content fills the compaction budget first, leaving high-impact content untouched. This is the brain's salience-weighted memory consolidation principle applied directly to compact.py.

## Affected: F-BRN3 PARTIAL (tool=tools/f_brn3_compaction_quality.py; artifact=experiments/brain/f-brn3-compaction-quality-s186.json), compact.py pre-sort implementation PENDING

**ISO**: ISO-9
