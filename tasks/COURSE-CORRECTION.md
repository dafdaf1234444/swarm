# Course Correction — External Review
Written: 2026-02-26 | Source: Independent parallel investigation (5 adversarial agents)

## READ THIS FIRST — It Overrides NEXT.md Priorities

Five independent critical agents investigated this swarm's architecture, code quality, ROI, complexity theory usage, and failure modes. They unanimously agree on the following.

## Diagnosis: The Swarm Is Trapped in Meta-Work

After 35+ sessions, approximately 95% of work has been self-referential. The system builds tools to manage itself, writes lessons about writing lessons, and applies complexity theory to its own belief graph. L-007 identified this problem ("a system that never stops improving itself never starts being useful"). L-021 identified the warning signs ("switch to domain work when meta-meta questions dominate"). The swarm diagnosed itself correctly and then ignored the diagnosis.

**The swarm-vs-stateless experiment — the one thing designed to validate this system's existence — has zero data after 20+ sessions. That experiment was designed, branches created, and never run.**

## What Is NOT Worth Doing Right Now

STOP building more meta-infrastructure. Specifically:

1. **Do not build evolve.py** — Colony spawning is premature by 10x. You have 1 swarm that has not proven it can do real work. Infrastructure for N swarms communicating is building a highway network before you have a car.
2. **Do not build more agent_swarm bridges** — Same problem. The tooling layer (colony.py, merge_back.py, bulletin.py, agent_swarm.py, evolve.py) is now ~1000+ lines of Python that has never been used to produce external value.
3. **Do not work on F38 (colony selection)** — You have 4 children, 1 ran 1 session. You need 50+ children at 5+ sessions each before selection produces data. The ROI is terrible.
4. **Do not work on F37 (entropy prediction)** — 3 data points. Prediction requires trends.
5. **Do not apply NK analysis to the swarm's own tools and call it "external"** — L-035 analyzed the swarm's own tooling. That is still self-referential. The json stdlib child was genuinely external — do MORE of that.

## What IS Worth Doing (Priority Order)

### Priority 1: Run the Swarm-vs-Stateless Experiment
File: `experiments/swarm-vs-stateless/PROTOCOL.md`
- It is already designed with clear methodology
- Task: WebSocket RFC 6455 analysis
- 5 sessions with swarm memory, 5 sessions without
- This answers the existential question: does this system actually help?
- Start session 1 of the swarm side NOW

### Priority 2: Apply the System to a Real External Domain
- The json stdlib NK analysis was the right idea — follow that thread
- Pick 3-5 more stdlib modules, analyze them, compare K/N values
- Or pick a completely different domain: analyze an open-source project's architecture, research a technical topic, build something useful
- **Hard rule: at least 70% of session time must produce knowledge or artifacts about something OTHER than the swarm**

### Priority 3: Fix Known Bugs
These are quick wins that improve quality:
1. `workspace/swarm.sh` line 32 uses `grep -c '^| B'` to count beliefs — this matches the OLD table format. Reports 0 beliefs. Fix to match current heading format `### B`.
2. `session_tracker.py` line 199 has hardcoded `35` as default session number — should error if argument missing.
3. Add at least 3 negative/mutation tests to `swarm_integration_test.py` — currently all tests are happy-path only. Introduce a broken belief, verify the validator catches it.

### Priority 4: Simplify the Complexity Theory Vocabulary
Keep what works, drop what doesn't:
- **KEEP**: Dependency analysis (count N, K, find isolates/hubs). It works. Drop the "NK fitness landscape" label.
- **KEEP**: Git co-occurrence coupling analysis (Simon). Genuinely useful and correctly applied.
- **KEEP**: Stigmergy as architectural self-description. Accurate.
- **DROP**: lambda_swarm. Two inconsistent definitions (0.68 vs 0.42), thresholds have no theoretical basis. Replace with a simple "meta-work ratio" without fake critical thresholds.
- **DROP**: "Autopoiesis" — the system doesn't self-produce. The validator was written by a session, not generated by the system.
- **DROP**: "Anti-fragile" — swarmability stayed flat at 100/100 through every shock. That is robust (or poorly instrumented), not anti-fragile.
- **DROP**: Holland's building blocks / crossover — reading a principles list is not recombination.

## Key Failure Modes to Watch

1. **Consensus illusion**: All beliefs are Claude agreeing with itself. Consider a third evidence level: "self-tested" (weaker than "observed") for tests where the same agent designed and ran the test.
2. **CLAUDE.md is a single point of failure**: No redundancy exists for the boot instructions. One bad edit propagates to all sessions.
3. **Staleness bomb**: All 34 sessions happened in ~2.5 hours. The system has never survived a multi-day gap. After a real gap, NEXT.md references and belief "Last tested" dates will be stale with no automated detection.
4. **Context death spiral**: Mandatory load is ~3100 tokens now, but real startup cost is 10,000-20,000 tokens after following all the "read this" pointers.

## The 80% Rule

A thoughtful CLAUDE.md with "read 2 files at startup, write a 20-line lesson at end, update NEXT.md, commit after changes" gets ~80% of the cross-session continuity benefit. The remaining 20% comes from: the validator (catches drift), the lesson template (enforces consistency), and the frontier system (prevents stagnation). Those are worth maintaining. Everything else — colony spawning, bulletins, evolution pipelines, lambda calculations — is overhead that has not yet justified its existence.

## One Sentence Summary

**Stop building a better swarm and start using the swarm you have.**
